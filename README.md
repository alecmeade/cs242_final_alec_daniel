# ML Based Video Compression Through Over Fitting
## Computing at Scale (CS-242) By HT Kung

### Authors
  * **Alec Meade** 
  * **Daniel Evangelakos** 

### Files
The files in this repository split the video compression pipeline into two stages. Model training and model compression (not video compression.
*  **[video_compression_model_trainer.ipynb](https://github.com/alecmeade/cs242_final_alec_daniel/blob/master/video_compression_model_trainer.ipynb):** This file trains two network architecture types. The first architecture using only the frame number as inputs to predict the output images and the second architecture is a variant of a U-Net. It is parametrized to allow for varying levels of "depth" or number of convultions between the encoder component of the network and the pinch layer. Networks and the intemediary image represenations whether they be frame numbers or the compressed activations of the middle layer of the second architecture and then output to files which can be read in by the deep_compression.ipynb pipeline below. The trainined models in this file are evaluated and trained across both classical and visual loss metrics including, MSE, CrossEntropy, VGG, SSIM and PSNR.

*  **[deep_compress.ipynb](https://github.com/alecmeade/cs242_final_alec_daniel/blob/master/deep_compression.ipynb):** This file implements a [deep compression pipeline](https://arxiv.org/pdf/1510.00149.pdf) and runs it on the decoder component of the neural networks generated by the model training file to evaluate the degradation of visual loss metrics and video compression quality as the network size is minimized.

### Video Data
The video data used to train the models used in this project comes from six individual uncompressed videos that are commonly used for testing video compression quality. They were selected to have varying degrees of detail, noise, motion in various parts of the image. All data used in this project has been uploaded to [Kaggle and can be found here](
https://www.kaggle.com/alecmeade/cs242-final). To download the data from Kaggle you will need to create a directory in google drive at "My Drive\Kaggle" and "My Drive\Kaggle\Datasets". Furthermore, within the kaggle directory you must store kaggle api key as seen in this [example](https://medium.com/@yvettewu.dw/tutorial-kaggle-api-google-colaboratory-1a054a382de0). The six datasets are as follows:

1. **Beauty**: 
  * Stationary camera and black background
  * Human face and movement

2. **Honey Bee**
  * Varied color palette
  * Stationary camera and background
  * Isolated section of high velocity movement
3. **Nature**
  * Muted color palette 
  * Lots of high frequency edges 
  * Non-linear camera motion
4. **Ready Set Go**
  * Stationary camera and background
  * Lots of unique movement
5. **Shake N Dry**
  * Stationary camera and background
  * Almost full screen distortion function 
6. **Yacht**
  * Changing low frequency background
  * Linear moving object in the front


### References
* [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/pdf/1510.00149.pdf)
* [Deep Generative Video Compression](https://papers.nips.cc/paper/9127-deep-generative-video-compression.pdf)
* [Image and Video Compression with Neural Networks: A Review](https://arxiv.org/pdf/1904.03567.pdf)
* [Distilling Knowledge From a Deep Pose Regressor Network](http://www.cs.ox.ac.uk/files/11078/ICCV19_Distilling_Knowledge_From_a_Deep_Pose_Regressor_Network.pdf)
* [Lossy Image Compression With Autoencoders](https://arxiv.org/pdf/1703.00395v1.pdf)
* [Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531.pdf)
* [An End-to-End Compression Framework Based on Convolutional Neural Networks](https://arxiv.org/pdf/1708.00838v1.pdf)
