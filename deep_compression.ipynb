{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_compression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNbc0kfAPJed0S84wiOkjZr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alecmeade/cs242_final_alec_daniel/blob/master/deep_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPS1prkr5UWT",
        "colab_type": "code",
        "outputId": "116a0c58-ff7f-454c-cf92-bea724588b33",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#@title Python Imports\n",
        "!pip install tensorflow_model_optimization\n",
        "\n",
        "import datetime\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import pickle\n",
        "import tempfile\n",
        "import scipy.sparse\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from enum import Enum\n",
        "from google.colab import drive\n",
        "# Mounts your personal google drive onto the current machine\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_model_optimization in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_model_optimization) (0.1.5)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow_model_optimization) (1.18.4)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow_model_optimization) (1.12.0)\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXVq1imI5Xhm",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def isWeightLayer(layer):\n",
        "  \"\"\"Determines whether a layer in a neural network has any weights.\n",
        "  Args:\n",
        "    layer: A tf.layer object\n",
        "  Returns:\n",
        "    A boolean indicating if the layer contains weights.\n",
        "  \"\"\"\n",
        "  weight_biases = layer.get_weights()\n",
        "  return len(weight_biases) > 0\n",
        "\n",
        "def getLayerName(layer):\n",
        "  \"\"\"Gets the name of a tensfor flow layer without any appended wrappers.\n",
        "  Args:\n",
        "    layer: A tf.layer object\n",
        "  Returns:\n",
        "    A cleaned str containing the layer name.\n",
        "  \"\"\"\n",
        "  return layer.name.replace('prune_low_magnitude_', '')\n",
        "\n",
        "def modelToCsr(model, nbit=None):\n",
        "  \"\"\"Converts a model to a list of compressed sparse row matrices.\n",
        "  Args:\n",
        "    model: A tf.Model\n",
        "    nbit: A number of bits to reduce the data type of the matrix. If none no\n",
        "      data type changes will take place. Only 32, 8 and None are supported. 8\n",
        "      bit converts to int8 to save additional space.\n",
        "  Returns:\n",
        "    A list of lists where the nested list contains the sparse row format \n",
        "    matrices of the weights and biases of the layer respectively. Also returns\n",
        "    a tuple containing (number of values in dense representation, num values in\n",
        "    sparse representation, percentage reduction of values).\"\"\"\n",
        "  \n",
        "  csr_weights = []\n",
        "  number_reductions = {}\n",
        "\n",
        "  # Track the overall compression due to transformation to CSR.\n",
        "  num_values_dense = 0\n",
        "  num_values_csr = 0\n",
        "  \n",
        "  # Determine if any dtype transformations are required.\n",
        "  csr_dtype = None\n",
        "  if nbit == 32:\n",
        "    csr_dtype = np.float32\n",
        "  elif nbit == 16:\n",
        "    csr_dtype = np.float16\n",
        "  elif nbit == 8:\n",
        "    csr_dtype = np.int8\n",
        "\n",
        "  for layer in model.layers:\n",
        "    if isWeightLayer(layer):\n",
        "      layer_name = getLayerName(layer)\n",
        "      csr_weights.append([])\n",
        "\n",
        "      for w_mtrx in layer.get_weights():\n",
        "        w = w_mtrx.reshape(-1)        \n",
        "        csr = None\n",
        "        if csr_dtype is not None:\n",
        "          if nbit == 8:\n",
        "            # If 8 bit is requested we convert to int8 representation which\n",
        "            # will later be scaled back to float8 when calling csrToModel.\n",
        "            scale = (2 ** nbit) - 1\n",
        "            w = np.round(w * scale)\n",
        "          csr = csr_matrix(w, dtype=csr_dtype)\n",
        "        else:\n",
        "          csr = csr_matrix(w)\n",
        "\n",
        "        num_values_dense += np.product(w_mtrx.shape)\n",
        "        num_values_csr += csr.nnz\n",
        "        csr_weights[-1].append(csr)\n",
        "  \n",
        "  # Determine the overall reduction in values from converting to CSR.\n",
        "  num_values_reduction = 100.0 * num_values_csr / num_values_dense\n",
        "\n",
        "  return csr_weights, (num_values_dense, num_values_csr, num_values_reduction)\n",
        "\n",
        "def csrToModel(csr_weights, model, nbit=None):\n",
        "  \"\"\"Converts a model to a list of compressed sparse row matrices.\n",
        "  Args:\n",
        "    csr_weights: A list of csr_matrices created by modelToCSR\n",
        "    model: A model to overwrite the layers.\n",
        "    nbit: Indicates the size of the provided csr_weights matrix dtype. Only 8\n",
        "      bit and None are supported. 8 bit requires a scaling from int8 to float32.\n",
        "  Returns:\n",
        "    A populated model created from the csr_weights and model_ctor.\"\"\"\n",
        "  \n",
        "  # Creates a new model.\n",
        "  dense_weights = []\n",
        "\n",
        "  # Converts csr matrices to dense representation.\n",
        "  for layer in csr_weights:\n",
        "    dense_weights.append([])\n",
        "    for csr_mtrx in layer:\n",
        "      if nbit == 16:\n",
        "        # Special case with np.float16 cannot be directly converted to dense.\n",
        "        csr_mtrx = csr_mtrx.astype(np.float32)\n",
        "\n",
        "      dense_mtrx = np.array(csr_mtrx.todense())\n",
        "      if nbit == 8:\n",
        "        # Scales the dense matrix to float 32 if it was compressed to int8.\n",
        "        scale = (2 ** nbit) - 1\n",
        "        dense_mtrx = dense_mtrx.astype(np.float32) / scale\n",
        "      dense_weights[-1].append(dense_mtrx)\n",
        "  \n",
        "  # Sets the weights in each corresponding layer of the created model.\n",
        "  i = 0\n",
        "  for layer in model.layers:\n",
        "    if isWeightLayer(layer):\n",
        "      layer_w = layer.get_weights()\n",
        "      for j in range(len(layer_w)):\n",
        "        layer_w[j] = dense_weights[i][j].reshape(layer_w[j].shape)\n",
        "      layer.set_weights(layer_w)\n",
        "      i += 1\n",
        "  return model\n",
        "\n",
        "class CSRHuffmanCoder():\n",
        "  \"\"\"An entropy encoder for a list of matrices generated by modelToCSR. \n",
        "  Specfically, the npz format utilizes gzip compression, a variant of Huffman\n",
        "  coding.\n",
        "  \"\"\"\n",
        "  def __init__(self, file_path_prefix=\"/tmp/model_%d\"):\n",
        "    self.file_paths = []\n",
        "    # The prefix to append to each file name for each matrix.\n",
        "    self.file_path_prefix = file_path_prefix\n",
        "\n",
        "  def encode(self, csr_weights):\n",
        "    \"\"\"Compresses a list of csr matrices.\n",
        "    Args:\n",
        "      csr_weights: A list of weight matrices created by modelToCSR.\n",
        "    Returns:\n",
        "      The total memory of the written files in kb.\n",
        "    \"\"\"\n",
        "    self.file_paths = []\n",
        "    total_kb = 0\n",
        "    idx = 0\n",
        "    for i, layer in enumerate(csr_weights):\n",
        "      self.file_paths.append([])\n",
        "      for j, mtrx in enumerate(layer):\n",
        "        mtrx_path = self.file_path_prefix % idx\n",
        "        self.file_paths[-1].append(mtrx_path)\n",
        "        scipy.sparse.save_npz(mtrx_path, mtrx)\n",
        "        total_kb += os.path.getsize(mtrx_path + '.npz')\n",
        "        idx += 1\n",
        "    return total_kb / 1000.0\n",
        "\n",
        "  def decode(self):\n",
        "    \"\"\"Decompresses a list of csr matrices.\n",
        "    Returns:\n",
        "      Converts all written npz files into a list of spare csr matrices that\n",
        "      match the format of csrToModel.\n",
        "    \"\"\"\n",
        "    csr_weights = []\n",
        "    for path_group in self.file_paths:\n",
        "      csr_weights.append([])\n",
        "      for path in path_group:\n",
        "        csr_weights[-1].append(scipy.sparse.load_npz(path + '.npz'))\n",
        "\n",
        "    return csr_weights\n",
        "\n",
        "def getSizeRecursive(path):\n",
        "  \"\"\"Gets the size of a directory in kb.\n",
        "  Args:\n",
        "    path: The file path to the directory.\n",
        "  Returns:\n",
        "    The size of all the directory contents in kb.\n",
        "  \"\"\"\n",
        "  total_size = 0\n",
        "  for dir_path, dir_names, file_names in os.walk(path):\n",
        "    for f in file_names:\n",
        "      fp = os.path.join(dir_path, f)\n",
        "      total_size += os.path.getsize(fp)\n",
        "  return total_size / 1000.0\n",
        "    \n",
        "def clearTempFiles(path='/tmp'):\n",
        "  \"\"\"Clears temporary compressed files and recreates the provided directory.\n",
        "  Args: \n",
        "    path: The path to the directory containing temp files.\n",
        "  \"\"\"    \n",
        "  if os.path.exists(path):\n",
        "    shutil.rmtree(path)\n",
        "\n",
        "  os.mkdir('/tmp')\n",
        "\n",
        "class Dataset():\n",
        "  \"\"\"Generic class to represent a dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, x_train, y_train, x_test, y_test, batch_size, seed=0):\n",
        "    self.x_train = x_train\n",
        "    self.y_train = y_train\n",
        "    self.x_test = x_test\n",
        "    self.y_test = y_test\n",
        "    self.batch_size = batch_size\n",
        "    self.num_train_samples = self.x_train.shape[0]\n",
        "\n",
        "    # Random seed created to allow consistency in testing.\n",
        "    self.seed = seed\n",
        "    self.train_dataset = self.convertDatasetToBatches()\n",
        "\n",
        "    # Epochs per steps is used throughout training to properly time different\n",
        "    # stages in the compression pipeline.\n",
        "    self.epoch_steps = np.floor(\n",
        "          1.0 * self.num_train_samples / self.batch_size\n",
        "        ).astype(np.int32)\n",
        "\n",
        "  def convertDatasetToBatches(self):\n",
        "    \"\"\"Splits the training dataset into batches.\"\"\"\n",
        "    tf.compat.v1.set_random_seed(self.seed)\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((self.x_train,\n",
        "                                                        self.y_train))\n",
        "    # Train dataset is shuffled with a consistent random seed and any remainder\n",
        "    # data is dropped to ensure all batches are equal in size.\n",
        "    train_dataset = train_dataset.shuffle(self.num_train_samples, \n",
        "                                           seed=self.seed, \n",
        "                                           reshuffle_each_iteration=True)\n",
        "    train_dataset = train_dataset.batch(self.batch_size, drop_remainder=True)\n",
        "    return train_dataset\n",
        "\n",
        "def loadDecoderModelAndData(folder_path, batch_size=None, nbit=8):\n",
        "  \"\"\"Loads a decoder model from drive along with its test and train data.\n",
        "  Args:\n",
        "    folder_path: The path to the decoder model in drive.\n",
        "  Returns:\n",
        "    A tuple containg a model and Dataset object.\n",
        "  \"\"\"\n",
        "  inputs = np.load(folder_path + \"input.npy\")\n",
        "  huffman = CSRHuffmanCoder('/tmp/inputs_%d')\n",
        "  w = inputs\n",
        "  scale = (2 ** nbit) - 1\n",
        "  w = np.round(w * scale)\n",
        "  w = w.reshape(-1)\n",
        "  print(\"Truncated Input Kb %d\" % \n",
        "        (huffman.encode([[csr_matrix(w, dtype=np.int8)]])))\n",
        "  tf.constant(huffman.decode()[0][0].astype(np.float32).todense() * scale ,\n",
        "              dtype=tf.float32)\n",
        "      \n",
        "  outputs = tf.constant(np.load(folder_path + \"output.npy\"), dtype=tf.float32)\n",
        "  model = dense_model = tf.keras.models.load_model(folder_path + \"model\")\n",
        "  x_train = inputs\n",
        "  y_train = outputs\n",
        "  x_test  = inputs\n",
        "  y_test  = outputs\n",
        "  if batch_size is None:\n",
        "    batch_size = x_train.shape[0]\n",
        "  return model, Dataset(x_train, y_train, x_test, y_test, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE5TriZH9XDN",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class MetricsTracker():\n",
        "  \"\"\"Tracks multiple metrics during model training and can generate plots.\"\"\"\n",
        "  def __init__(self, metrics=None):\n",
        "    self.metrics = defaultdict(list)\n",
        "    # If metrics are provided as an argument populate them.\n",
        "    if metrics is not None:\n",
        "      for k, v in metrics.items():\n",
        "        self.metrics[k] = v\n",
        "    \n",
        "    self.step_markers = {}\n",
        "\n",
        "  def printMetricsAtIndex(self, metrics, index):\n",
        "    \"\"\"Prints the metric at the provided index from a list of provided metrics.\n",
        "    Args:\n",
        "      metrics: A list of strs with metric names.\n",
        "      index: The index from which to retrieve the metrics.\n",
        "    \"\"\"\n",
        "    metric_string = \"\"\n",
        "    for m_name in metrics:\n",
        "      if m_name in self.metrics:\n",
        "        m_value = self.metrics[m_name][index]\n",
        "        metric_string += \"%s: %0.2f | \" % (m_name, m_value)\n",
        "    print (metric_string)\n",
        "    \n",
        "  def concatLayer(self, metric_type, layer_name):\n",
        "    \"\"\"Creates a unique metric name that includes the layer name.\n",
        "    Args:\n",
        "      metric_type: The name of the metric.\n",
        "      layer_name: The name of the layer for which the metric corresponds.\n",
        "    Returns:\n",
        "      The layer specific metric name.\n",
        "    \"\"\"\n",
        "    return \"%s_%s\" % (metric_type, layer_name)\n",
        "\n",
        "  def addMetric(self, metric_type, val, layer_name=None):\n",
        "    \"\"\"Appends a new value for a given metric.\n",
        "    Args:\n",
        "      metric_type: The metric name to add the value to.\n",
        "      val: The value of the metric.\n",
        "      layer_name: The optional layer that the metric is calculated over.\n",
        "    \"\"\"\n",
        "    if layer_name is not None:\n",
        "      metric_type = self.concatLayer(metric_type, layer_name)\n",
        "\n",
        "    self.metrics[metric_type].append(val)\n",
        "\n",
        "  def addStepMarker(self, marker_name, step):\n",
        "    \"\"\"Adds a key value pair marker used for vertical lines on plots.\n",
        "    Args:\n",
        "      marker_name: The name of the event.\n",
        "      step: The training step the event occured on.\n",
        "    \"\"\"\n",
        "    self.step_markers[marker_name] = step\n",
        "\n",
        "  def getMetrics(self, in_filter=None, ex_filter=None):\n",
        "    \"\"\"Gets a list of metrics tracked by the tracker.\n",
        "    Args:\n",
        "      in_filter: A string that retrieved metrics must contain.\n",
        "      ex_filter: A string that retrieved metrics must not contain.\n",
        "    Returns:\n",
        "      A list of metric names as strings.\n",
        "    \"\"\"\n",
        "    metric_names = []\n",
        "    for m in self.metrics.keys():\n",
        "      add_name = True\n",
        "\n",
        "      # Inclusive filters.\n",
        "      if in_filter is not None and in_filter not in m: \n",
        "        add_name = False\n",
        "\n",
        "      # Exclusive filters.\n",
        "      if ex_filter is not None and ex_filter in m:   \n",
        "        add_name = False\n",
        "      \n",
        "      if add_name:\n",
        "        metric_names.append(m)\n",
        "\n",
        "    return metric_names\n",
        "\n",
        "  def plotMetrics(self, title, y_metrics, xlim = None, ylim = None, \n",
        "                  show_markers = True):\n",
        "    \"\"\"Plots a time series of a set of metrics that the tracker contains.\n",
        "    Args:\n",
        "      title: The plot title.\n",
        "      y_metrics: The names of the y_metrics that the tracker has.\n",
        "      xlim: Optional array of [min, max] for x vals.\n",
        "      ylim: Optional array of [min, max] for y vals.\n",
        "      show_markers: Whether or not to show marked events as vertical lines.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(25, 5))\n",
        "    plt.title(title)\n",
        "    for m in y_metrics:\n",
        "      plt.plot([i + 1 for i in range(len(self.metrics[m]))],\n",
        "               self.metrics[m], label=m)\n",
        "    plt.legend()\n",
        "    \n",
        "    # Display marked events as vertical lines.\n",
        "    if show_markers:\n",
        "      for i, (k, v) in enumerate(self.step_markers.items()):\n",
        "        plt.axvline(v, label=\"%d | %s\" % (i, k), linestyle='dashed')\n",
        "\n",
        "    if xlim is not None:\n",
        "      plt.xlim(xlim)\n",
        "\n",
        "    if ylim is not None:\n",
        "      plt.ylim(ylim)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plotFrames(golden_frames, predicted_frames):\n",
        "  frame_shape = golden_frames.shape\n",
        "  num_images = int(frame_shape[0])\n",
        "  fig, axes = plt.subplots(num_images, 3, figsize=(30, 30))\n",
        "  for i in range(num_images):\n",
        "    golden_image = golden_frames[i]\n",
        "    axes[i, 0].imshow(golden_image)\n",
        "    predicted_image = predicted_frames[i]\n",
        "    axes[i, 1].imshow(predicted_image)\n",
        "    diff_image = tf.math.abs(golden_image - predicted_image)\n",
        "    axes[i, 2].imshow(diff_image)\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRJXU0d4Z-hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the psnr error between the golden frames and the predicted frames.\n",
        "def psnrLoss(y_true, y_pred, as_loss=True):\n",
        "  '''\n",
        "  y_true: A tf.tensor representing the set of original sequential frames.\n",
        "  y_pred: A tf.tensor reprsenting the set of predicted frames. \n",
        "\n",
        "  return: The psnr between the two transformed in some way to fit with the \n",
        "  understanding of error.\n",
        "  '''\n",
        "  psnr = tf.image.psnr(y_true, y_pred, max_val = 1.0)\n",
        "  \n",
        "  if as_loss:\n",
        "    return 50 - psnr\n",
        "  else:\n",
        "    return psnr\n",
        "\n",
        "# Calculate the ssim error between the golden frames and the predicted frames.\n",
        "def ssimLoss(y_true, y_pred, as_loss=True):\n",
        "  '''\n",
        "  y_true: A tf.tensor representing the set of original sequential frames.\n",
        "  y_pred: A tf.tensor reprsenting the set of predicted frames. \n",
        "\n",
        "  return: The sse between the two transformed in some way to fit with the \n",
        "  understanding of error.\n",
        "  '''\n",
        "  ssim = tf.image.ssim(y_true, y_pred, max_val = 1)\n",
        "\n",
        "  if as_loss:\n",
        "    return 1 - ssim\n",
        "  else:\n",
        "    return ssim\n",
        "\n",
        "# Load the vgg model globally so that you do not need to load it\n",
        "# everytime. If there is a way to define a static variable in python I will use \n",
        "# that instead.\n",
        "vgg_model = tf.keras.applications.VGG16(include_top=False,\n",
        "                                        input_shape=(1024,2048,3))\n",
        "vgg_model.trainable = False\n",
        "\n",
        "# Calculate the vgg error between the golden frames and the predicted frames.\n",
        "def vggLoss(y_true, y_pred, as_loss=True):\n",
        "  '''\n",
        "  y_true: A tf.tensor representing the set of original sequential frames.\n",
        "  y_pred: A tf.tensor reprsenting the set of predicted frames. \n",
        "\n",
        "  return: The vgg between the two transformed in some way to fit with the \n",
        "  understanding of error.\n",
        "  '''\n",
        "  # golden_output = vgg_model(y_true)\n",
        "  # predicted_output = vgg_model(y_pred)\n",
        "\n",
        "  # diff_frames = predicted_output - golden_output\n",
        "  # return tf.math.reduce_mean(tf.math.square(diff_frames))\n",
        "  return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOq0fErg5dxT",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class DeepCompressor():\n",
        "  \"\"\"A class that implements a deep compression pipeline from \n",
        "    https://arxiv.org/pdf/1510.00149.pdf without Huffman coding.\"\"\"\n",
        "  def __init__(self, \n",
        "               model, \n",
        "               dataset,\n",
        "               loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                   from_logits=True),\n",
        "               epochs = 10, \n",
        "               pruning_start_stop = None, \n",
        "               quantization_start_stop = None,\n",
        "               prune_frequency = 0.1, \n",
        "               initial_sparsity = 0.1,\n",
        "               final_sparsity =0.7, \n",
        "               num_clusters = 16,\n",
        "               floating_point_bits = None):\n",
        "    \"\"\"Intializes the deep compressor.\n",
        "    Args:\n",
        "      model: A tensorflow model to compress.\n",
        "      dataset: A Dataset class object containg test and train data.\n",
        "      loss_fn: A loss function for training the model.\n",
        "      epochs: The number of epochs to run the compress:\n",
        "      pruning_start_stop: An array of the start and stop epochs for pruning of\n",
        "        weights. If None, no pruning occurs.\n",
        "      quantization_start_stop: An array of the start and stop epochs for \n",
        "        quantization. If None no quantization occurs.\n",
        "      prune_frequency: A decimal indicating the fraction of an epoch with which\n",
        "        to reprune the network.\n",
        "      initial_sparsity: An initial sparsity to begin pruning [0, 1].\n",
        "      final_sparsity: The final sparsity in the network after pruning [0, 1].\n",
        "      num_clusters: The number of weight clusters to maintain after \n",
        "        quantization. This is determined per layer.\n",
        "      floating_point_bits: The number of bits to truncate values in the network\n",
        "        to. If None then no truncation occurs.\n",
        "    \"\"\"\n",
        "\n",
        "    print (\"Intializing.\")\n",
        "    self.data = dataset\n",
        "    self.model = model\n",
        "    self.loss_fn = loss_fn\n",
        "    self.step = 0\n",
        "    self.epochs = epochs\n",
        "\n",
        "    # Adjsut pruning start and stop to be in steps instead of epochs.\n",
        "    self.pruning_start_stop = None\n",
        "    if pruning_start_stop is not None:\n",
        "      self.pruning_start_stop = [int(pruning_start_stop[0] * self.data.epoch_steps),\n",
        "                                 int(pruning_start_stop[1] * self.data.epoch_steps)]\n",
        "\n",
        "    # Setup pruning parameters.\n",
        "    self.pruning_params = {}\n",
        "    self.prune_frequency = int(prune_frequency * self.data.epoch_steps)\n",
        "    self.initial_sparsity = initial_sparsity\n",
        "    self.final_sparsity = final_sparsity\n",
        "    self.prune_masks = {}\n",
        "    if self.pruning_start_stop is not None:\n",
        "      prune_params = {\n",
        "          'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
        "              initial_sparsity = self.initial_sparsity,\n",
        "              final_sparsity = self.final_sparsity,\n",
        "              begin_step = self.pruning_start_stop[0],\n",
        "              end_step = self.pruning_start_stop[1],\n",
        "              frequency = self.prune_frequency)\n",
        "      }\n",
        "      # Create pruning wrapper.\n",
        "      self.model = tfmot.sparsity.keras.prune_low_magnitude(self.model,\n",
        "                                                            **prune_params)\n",
        "\n",
        "    # Reset model optimizer after pruning wrapper is in place since it is\n",
        "    # removed.\n",
        "    self.model.optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Adjsut quantization start and stop to be in steps instead of epochs.\n",
        "    self.quantization_start_stop = None\n",
        "    if quantization_start_stop is not None:\n",
        "      self.quantization_start_stop = [int(quantization_start_stop[0] * self.data.epoch_steps),\n",
        "                                      int(quantization_start_stop[1] * self.data.epoch_steps)]\n",
        "      \n",
        "    # Assign floating bits and check for valid input.\n",
        "    self.num_clusters = num_clusters\n",
        "    self.floating_point_bits = floating_point_bits\n",
        "    assert self.floating_point_bits in [None, 8, 16, 32]\n",
        "    \n",
        "    # Walk the layers in the model and determine which have trainable weights.\n",
        "    # Also store a mapping between trainable vars in layer names for use during\n",
        "    # training to map gradients to the proper layers.\n",
        "    self.weight_cluster_map = {}\n",
        "    self.weight_layers = {}\n",
        "    self.trainable_vars_layer_map = {}\n",
        "    for layer in model.layers: \n",
        "      layer_name = getLayerName(layer)\n",
        "      if isWeightLayer(layer):\n",
        "        # Only include weight layers.\n",
        "        self.weight_layers[layer_name] = True\n",
        "\n",
        "      for v in layer.trainable_variables: \n",
        "        if self.isTrainableVarWeight(v):\n",
        "          self.trainable_vars_layer_map[v.name] = layer_name\n",
        "\n",
        "    # Initialize pruning masks with all ones.\n",
        "    self.createPruneMasks()\n",
        "\n",
        "    # Define metric names for metrics tracked by the compressor.\n",
        "    self.TEST_LOSS       = \"TEST_LOSS\"\n",
        "    self.TEST_ACC        = \"TEST_ACC\"\n",
        "    self.TEST_PSNR       = \"TEST_PSNR\"\n",
        "    self.TEST_SSIM       = \"TEST_SSIM\"\n",
        "    self.TEST_VGG        = \"TEST_VGG\"\n",
        "\n",
        "    self.TRAIN_LOSS      = \"TRAIN_LOSS\"\n",
        "    self.TRAIN_ACC       = \"TRAIN_ACC\"\n",
        "    self.TRAIN_PSNR      = \"TRAIN_PSNR\"\n",
        "    self.TRAIN_SSIM      = \"TRAIN_SSIM\"\n",
        "    self.TRAIN_VGG       = \"TRAIN_VGG\"\n",
        "\n",
        "    self.SPARSITY        = \"SPARSITY\"\n",
        "    self.NUM_WEIGHTS     = \"NUM_WEIGHTS\"\n",
        "    self.UNIQUE_WEIGHTS  = \"UNIQUE_WEIGHTS\"\n",
        "    self.MODEL_SIZE      = \"MODEL_SIZE\"\n",
        "  \n",
        "    self.metrics_tracker = MetricsTracker()\n",
        "    print (\"Intializing Complete.\")\n",
        "\n",
        "  def updateLayerMetrics(self):\n",
        "    \"\"\"Calculates layer specific metrics i.e, sparsity, num_weights and \n",
        "      unique_weights and appends them to the tracker.\"\"\"\n",
        "\n",
        "    all_layer_non_zero = 0\n",
        "    all_layer_total = 0\n",
        "    all_unique_w = [] \n",
        "    for layer in self.model.layers:\n",
        "      layer_name = getLayerName(layer)\n",
        "      if layer_name in self.weight_layers:\n",
        "        weights = layer.get_weights()[0]\n",
        "        non_zero = np.count_nonzero(weights)\n",
        "        total = np.prod(weights.shape)\n",
        "        all_layer_non_zero += non_zero\n",
        "        all_layer_total += total\n",
        "        sparsity = (total - non_zero) / total\n",
        "        unique_w = np.unique(weights)  \n",
        "        all_unique_w += list(unique_w)\n",
        "\n",
        "        # Add layer specific metrics to the tracker.\n",
        "        self.metrics_tracker.addMetric(self.SPARSITY, sparsity, layer_name)\n",
        "        self.metrics_tracker.addMetric(self.NUM_WEIGHTS, total, layer_name)\n",
        "        self.metrics_tracker.addMetric(self.UNIQUE_WEIGHTS, len(unique_w),\n",
        "                                       layer_name)\n",
        "    \n",
        "    # Roll up layer specific metrics and compute them for the entire network.\n",
        "    sparsity = (all_layer_total - all_layer_non_zero) / all_layer_total\n",
        "    all_unique_w = np.unique(all_unique_w)\n",
        "    self.metrics_tracker.addMetric(self.SPARSITY, sparsity, \"ALL\")\n",
        "    self.metrics_tracker.addMetric(self.NUM_WEIGHTS, all_layer_total, \"ALL\")\n",
        "    self.metrics_tracker.addMetric(self.UNIQUE_WEIGHTS, len(all_unique_w), \"ALL\")\n",
        "\n",
        "  def weights_kmeans(self, num_clusters, weights):\n",
        "    \"\"\"Runs a standard kmeans across weights with linear centroid intialization.\n",
        "    Args:\n",
        "      num_clusters: The number of kmeans clusters to create.\n",
        "      weights: A matrix containing weights to flatten and cluster.\n",
        "    Returns:\n",
        "      A tuple of weights now mapped to their centroids and corresponding matrix\n",
        "      containing class labels for centroids.\n",
        "    \"\"\"\n",
        "\n",
        "    # Only cluster non-zero weights.\n",
        "    non_zero_weights = weights[(weights != 0)]\n",
        "    min_w = np.min(non_zero_weights)\n",
        "    max_w = np.max(non_zero_weights)\n",
        "\n",
        "    # Linear intialization of clusters.\n",
        "    num_clusters = min(num_clusters, len(np.unique(non_zero_weights)))\n",
        "    centroids = np.linspace(min_w, max_w, num_clusters).reshape(-1, 1)\n",
        "    flat_weights = non_zero_weights.reshape(-1, 1)\n",
        "\n",
        "    # Train kmeans transformer and fit the weights.\n",
        "    kmeans_transformer = KMeans(n_clusters=num_clusters, init=centroids)\n",
        "    kmeans_transformer.fit(flat_weights)\n",
        "    centroid_labels = kmeans_transformer.predict(flat_weights)\n",
        "    centroids = kmeans_transformer.cluster_centers_\n",
        "    clustered_weights = weights\n",
        "    clustered_weights[(weights != 0)] = centroids[centroid_labels].reshape(-1)\n",
        "\n",
        "    cluster_labels = np.zeros(weights.shape) - 1\n",
        "    cluster_labels[(weights != 0)] = centroid_labels\n",
        "    return clustered_weights, cluster_labels\n",
        "\n",
        "  def validateModel(self):\n",
        "    \"\"\"Validates the current model against the test data and calculates all\n",
        "    corresponding test output metrics.\n",
        "    \"\"\"\n",
        "    test_loss = tf.keras.metrics.Mean()\n",
        "    test_psnr = tf.keras.metrics.Mean()\n",
        "    test_ssim = tf.keras.metrics.Mean()\n",
        "    test_vgg = tf.keras.metrics.Mean()\n",
        "    pred = self.model(self.data.x_test, training=False)\n",
        "    \n",
        "    loss = self.loss_fn(y_true=self.data.y_test, y_pred=pred)\n",
        "    psnr = psnrLoss(self.data.y_test, pred, False)\n",
        "    ssim = ssimLoss(self.data.y_test, pred, False)\n",
        "    vgg = vggLoss(self.data.y_test, pred, False)\n",
        "\n",
        "    test_loss.update_state(loss) \n",
        "    test_psnr.update_state(psnr) \n",
        "    test_ssim.update_state(ssim) \n",
        "    test_vgg.update_state(vgg) \n",
        "\n",
        "    # Append the metric for each epoch step so that test and train data have\n",
        "    # a consistent number of samples.\n",
        "    for i in range(0, self.data.epoch_steps):\n",
        "      self.metrics_tracker.addMetric(self.TEST_LOSS, test_loss.result())\n",
        "      self.metrics_tracker.addMetric(self.TEST_PSNR, test_psnr.result())\n",
        "      self.metrics_tracker.addMetric(self.TEST_SSIM, test_ssim.result())\n",
        "      self.metrics_tracker.addMetric(self.TEST_VGG, test_vgg.result())\n",
        "\n",
        "  def createPruneMasks(self):\n",
        "    \"\"\"Creates the initial masks for each weight layer of the network to \n",
        "    enforce that pruned weights remain pruned during training. The intial\n",
        "    prune masks contain all ones and therefore prune nothing.\"\"\"\n",
        "    for layer in self.model.layers:\n",
        "      layer_name = getLayerName(layer)\n",
        "      if layer_name in self.weight_layers:\n",
        "        weights = layer.get_weights()[0]\n",
        "        self.prune_masks[layer_name] = np.ones(weights.shape)\n",
        "\n",
        "  def updatePruneMasks(self):\n",
        "    \"\"\"Updates the set of prune masks across layers to mask all weights that\n",
        "    are zero.\"\"\"\n",
        "    for layer in self.model.layers:\n",
        "      layer_name = getLayerName(layer)\n",
        "      if layer_name in self.weight_layers:\n",
        "        weights = layer.get_weights()[0]\n",
        "        self.prune_masks[layer_name] = (weights != 0)\n",
        "\n",
        "  def reapplyPruneMasks(self):\n",
        "    \"\"\"Applies the currently stored set of pruning masks to the network \n",
        "    enforcing that all weights that were previously pruned are reset to 0.\"\"\"\n",
        "    for i, layer in enumerate(self.model.layers):\n",
        "      layer_name = getLayerName(layer)\n",
        "      if layer_name in self.prune_masks:\n",
        "        weights_biases = layer.get_weights()\n",
        "        weights_biases[0] = weights_biases[0] * self.prune_masks[layer_name]\n",
        "        self.model.layers[i].set_weights(weights_biases)\n",
        "\n",
        "  def fixBitWeights(self, nbit):\n",
        "    \"\"\"Truncates weights and biases in each layer of the network to nbits.\n",
        "    Args:\n",
        "      nbit: The number of weights to fix the network to.\n",
        "    \"\"\"\n",
        "    for i, layer in enumerate(self.model.layers):\n",
        "      layer_name = getLayerName(layer)\n",
        "      if layer_name in self.weight_layers:\n",
        "        weights_biases = layer.get_weights()\n",
        "        for j, w in enumerate(weights_biases):\n",
        "          weights_biases[j] = self.scaleMtrx(weights_biases[j], nbit)\n",
        "        \n",
        "        # Assign the truncated weights to the model.\n",
        "        self.model.layers[i].set_weights(weights_biases)\n",
        "\n",
        "  def scaleMtrx(self, mtrx, nbit):\n",
        "    \"\"\"Scales a matrix to nbit representation.\n",
        "    Args:\n",
        "      mtrx: A np.array or compatiable matrix of floats.\n",
        "      nbit: The number of bits to scale the matrix to.\n",
        "    Returns:\n",
        "      A new copy of a scaled matrix.\n",
        "    \"\"\"\n",
        "    scale = (2 ** nbit) - 1\n",
        "    return tf.math.round(mtrx * scale) / scale\n",
        "\n",
        "  def updateWeightClusters(self):\n",
        "    \"\"\"Updates each layer of the model to have shared weight clusters. If the\n",
        "    shared weight clusters are already created, then weights assigned to the\n",
        "    same centroid are re-averaged to produce a new centroid. This shifting of\n",
        "    centroids can occur due to momentum when training the model.\n",
        "    \"\"\"\n",
        "\n",
        "    for i, layer in enumerate(self.model.layers):\n",
        "      layer_name = getLayerName(layer)\n",
        "      if layer_name in self.weight_layers:\n",
        "        weights_biases = layer.get_weights()\n",
        "        if layer_name not in self.weight_cluster_map:\n",
        "          # Cluster each weight layer into centroids and store the centroid\n",
        "          # mapping if it has not been created before.\n",
        "          clustered_weights, centroid_labels = self.weights_kmeans(\n",
        "              self.num_clusters, \n",
        "              weights_biases[0])\n",
        "        \n",
        "          centroid_labels = centroid_labels.reshape(clustered_weights.shape) \n",
        "          self.weight_cluster_map[layer_name] = centroid_labels\n",
        "          \n",
        "          # Only cluster / quantize weights since biases are too few.\n",
        "          weights_biases[0] = self.prune_masks[layer_name] * clustered_weights\n",
        "\n",
        "        else:\n",
        "          # The centroid mapping for weights in the layer already exists. \n",
        "          # The mapping to centroids is used to update the centroid value with\n",
        "          # the new average of the cluster to ensure that the weights \n",
        "          # remain shared.\n",
        "          centroid_labels = self.weight_cluster_map[layer_name]\n",
        "          clustered_weights = np.zeros(weights_biases[0].shape)\n",
        "          for label in np.unique(centroid_labels):\n",
        "            if label == -1:\n",
        "              # Indicates a zero weight label\n",
        "              continue\n",
        "\n",
        "            centroid_mask = (centroid_labels == label)\n",
        "            centroid_count = np.sum(centroid_mask)\n",
        "            if centroid_count != 0:\n",
        "              # Only updates the centroid if it has corresponding weights.\n",
        "              centroid = np.sum(weights_biases[0] * centroid_mask) / centroid_count\n",
        "              clustered_weights += (self.prune_masks[layer_name] *\n",
        "                                    centroid_mask * centroid)\n",
        "\n",
        "          weights_biases[0] = clustered_weights\n",
        "\n",
        "        # Assign clustered weights back to the model.\n",
        "        self.model.layers[i].set_weights(weights_biases)\n",
        "\n",
        "  def accumulateSharedWeightGradients(self, gradients):\n",
        "    \"\"\"Determines the new gradients update for shared weights following the \n",
        "    specifications in https://arxiv.org/pdf/1510.00149.pdf. Specifically,\n",
        "    the gradients of shared weights are added together.\"\"\"\n",
        "\n",
        "    for j, var in enumerate(self.model.trainable_variables):\n",
        "      if var.name in self.trainable_vars_layer_map:\n",
        "        layer_name = self.trainable_vars_layer_map[var.name]\n",
        "        if layer_name in self.weight_cluster_map:\n",
        "          np_grads = np.zeros(gradients[j].shape)\n",
        "          centroid_labels = self.weight_cluster_map[layer_name]\n",
        "          prune_mask = self.prune_masks[layer_name]\n",
        "\n",
        "          for label in np.unique(centroid_labels):\n",
        "            # For each centroid determine the total gradients by summing the \n",
        "            # gradients across all weights.\n",
        "\n",
        "            if label == -1:\n",
        "              # Indicates a zero weight label\n",
        "              continue\n",
        "\n",
        "            centroid_mask = (centroid_labels == label)\n",
        "            centroid_update = np.sum(gradients[j] * centroid_mask * prune_mask)              \n",
        "            np_grads[centroid_mask] = centroid_update\n",
        "\n",
        "          # Update the existing computed gradients to reflect that gradients\n",
        "          # for shared weights are cummulative.\n",
        "          gradients[j] = np_grads\n",
        "\n",
        "    return gradients \n",
        "\n",
        "  def isTrainableVarWeight(self, trainable_var):\n",
        "    \"\"\"Determines if a variable is a weight or bias.\n",
        "    Args:\n",
        "      trainable_var: A tf trainable var.\n",
        "    Returns:\n",
        "      True if the variable represents weights, false if biases.\n",
        "    \"\"\"\n",
        "    return 'kernel' in trainable_var.name\n",
        "\n",
        "  def train_and_compress(self):\n",
        "    self.step = 0\n",
        "    # Create pruning call backs.\n",
        "    prune_cb = tfmot.sparsity.keras.UpdatePruningStep()\n",
        "    prune_cb.set_model(self.model)\n",
        "    prune_cb.on_train_begin()\n",
        "\n",
        "    for epoch in range(self.epochs):    \n",
        "      print (\"\\n*****************\\nEpoch: %d | Step: %d\" % (epoch, self.step))\n",
        "      prune_cb.on_epoch_begin(epoch)\n",
        "      \n",
        "      for i, (x_batch, y_batch) in enumerate(self.data.train_dataset):\n",
        "        prune_cb.on_train_batch_begin(batch=i)\n",
        "        train_loss = tf.keras.metrics.Mean()\n",
        "        train_psnr = tf.keras.metrics.Mean()\n",
        "        train_ssim = tf.keras.metrics.Mean()\n",
        "        train_vgg = tf.keras.metrics.Mean()\n",
        "\n",
        "        #####################\n",
        "        # Calculate Loss\n",
        "        #####################\n",
        "        pred = None\n",
        "        loss = None\n",
        "        psnr = None\n",
        "        ssim = None\n",
        "        vgg  = None\n",
        "        with tf.GradientTape() as tape:\n",
        "          # Compute all loss metrics on the current training batch.\n",
        "          pred = self.model(x_batch, training=True)\n",
        "          loss = self.loss_fn(y_true=y_batch, y_pred=pred)\n",
        "          psnr = psnrLoss(y_batch, pred, False)\n",
        "          ssim = ssimLoss(y_batch, pred, False)\n",
        "          vgg = vggLoss(y_batch, pred, False)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        #####################\n",
        "        # Pruning Weights\n",
        "        #####################\n",
        "        if (self.pruning_start_stop is not None and \n",
        "            self.pruning_start_stop[0] <= self.step \n",
        "            and self.pruning_start_stop[1] > self.step):\n",
        "          # Only prune between start and stop epochs.\n",
        "\n",
        "          if self.pruning_start_stop[0] == self.step:\n",
        "            print(\"Pruning | Step: %d\" % self.step)\n",
        "            self.metrics_tracker.addStepMarker(\"PRUNE_START\", self.step)\n",
        "\n",
        "          self.updatePruneMasks()\n",
        "\n",
        "          if self.pruning_start_stop[1] == self.step + 1:\n",
        "            print(\"Pruning Complete | Step: %d\" % (self.step + 1))\n",
        "            self.metrics_tracker.addStepMarker(\"PRUNE_END\", self.step + 1)\n",
        "        #####################\n",
        "        # Quantizing Weights\n",
        "        #####################\n",
        "        if (self.quantization_start_stop is not None and \n",
        "            self.quantization_start_stop[0] <= self.step \n",
        "            and self.quantization_start_stop[1] > self.step):\n",
        "          # Only quantize between start and stop epochs.\n",
        "\n",
        "          if self.quantization_start_stop[0] == self.step:\n",
        "            print(\"Quantizing | Step: %d\" % self.step)\n",
        "            self.metrics_tracker.addStepMarker(\"QUANT_START\", self.step)\n",
        "            # Creates the initial set of weight clusters only once.\n",
        "            self.updateWeightClusters()\n",
        "          \n",
        "          # Update gradients to account for shared weights across the matrix.\n",
        "          gradients = self.accumulateSharedWeightGradients(gradients)\n",
        "\n",
        "          if self.quantization_start_stop[1] == self.step + 1:\n",
        "            self.metrics_tracker.addStepMarker(\"QUANT_END\", self.step + 1)\n",
        "\n",
        "        #####################\n",
        "        # Reduce Bits\n",
        "        #####################\n",
        "        if self.floating_point_bits is not None:\n",
        "          self.fixBitWeights(self.floating_point_bits)\n",
        "\n",
        "        #####################\n",
        "        # Updating Gradients\n",
        "        #####################  \n",
        "        # Apply the gradient update. These gradients have possibly been altered\n",
        "        # by accumulateSharedWeightGradients in the quantization step.\n",
        "        self.model.optimizer.apply_gradients(zip(gradients, \n",
        "                                                self.model.trainable_variables))\n",
        "\n",
        "        if self.pruning_start_stop is not None:\n",
        "          # Pruning must occur again after apply gradients because the tf\n",
        "          # pruning cuts out after pruning_stop step and even zero weights can\n",
        "          # be updated due to momentum in the optimizer.\n",
        "          self.reapplyPruneMasks()\n",
        "\n",
        "        if (self.quantization_start_stop is not None and \n",
        "            self.quantization_start_stop[0] <= self.step \n",
        "            and self.quantization_start_stop[1] > self.step):\n",
        "          # Regroup weight clusters to ensure that shared weights are maintained\n",
        "          # after the gradient update.\n",
        "          self.updateWeightClusters()\n",
        "\n",
        "\n",
        "        # Update training metrics at each step.\n",
        "        train_loss.update_state(loss) \n",
        "        train_psnr.update_state(psnr) \n",
        "        train_ssim.update_state(ssim) \n",
        "        train_vgg.update_state(vgg) \n",
        "            \n",
        "        self.metrics_tracker.addMetric(self.TRAIN_LOSS, train_loss.result())\n",
        "        self.metrics_tracker.addMetric(self.TRAIN_PSNR, train_psnr.result())\n",
        "        self.metrics_tracker.addMetric(self.TRAIN_SSIM, train_ssim.result())\n",
        "        self.metrics_tracker.addMetric(self.TRAIN_VGG, train_vgg.result())\n",
        "  \n",
        "        self.updateLayerMetrics()\n",
        "        self.step += 1\n",
        "\n",
        "      prune_cb.on_epoch_end(epoch)\n",
        "\n",
        "      #####################\n",
        "      # Validating Model\n",
        "      #####################\n",
        "      self.validateModel()\n",
        "      self.metrics_tracker.printMetricsAtIndex([self.TRAIN_LOSS,\n",
        "                                                self.TEST_LOSS], \n",
        "                                               epoch * self.data.epoch_steps)\n",
        "      \n",
        "    # Ensure that quantization is the final step of the deep compression \n",
        "    # pipeline to make sure that no weights deviate from their shared centroids.\n",
        "    if self.quantization_start_stop is not None:\n",
        "      assert self.step == self.quantization_start_stop[1]\n",
        "\n",
        "    prune_cb.on_train_end()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQZs2eS40A8L",
        "colab_type": "code",
        "outputId": "4299f432-a58b-4ca6-ad62-de631ae616f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "###################\n",
        "# Deep Compression\n",
        "###################\n",
        "MODEL_PATH = \"HoneyBeePSNR\"\n",
        "DATA_DIR = \"/content/gdrive/My Drive/Data\"\n",
        "model, dataset = loadDecoderModelAndData(DATA_DIR + \"/\" + MODEL_PATH+ \"/\", \n",
        "                                         batch_size=1, \n",
        "                                         nbit=8)\n",
        "loss_fn = psnrLoss\n",
        "floating_point_bits = 16\n",
        "dc = DeepCompressor(\n",
        "    model,\n",
        "    dataset,\n",
        "    loss_fn = loss_fn,\n",
        "    epochs = 150, \n",
        "    pruning_start_stop = [10, 80], \n",
        "    quantization_start_stop = [100, 150],\n",
        "    prune_frequency = 1, \n",
        "    initial_sparsity = 0.1,\n",
        "    final_sparsity = 0.6, \n",
        "    num_clusters = 8,\n",
        "    floating_point_bits = floating_point_bits)\n",
        "dc.train_and_compress()\n",
        "\n",
        "#######################\n",
        "# Plot Tracked Metrics\n",
        "#######################\n",
        "dc.metrics_tracker.plotMetrics(\"Loss\", [dc.TRAIN_LOSS, dc.TEST_LOSS])\n",
        "dc.metrics_tracker.plotMetrics(\"PSNR\", [dc.TRAIN_PSNR, dc.TEST_PSNR])\n",
        "dc.metrics_tracker.plotMetrics(\"SSIM\", [dc.TRAIN_SSIM, dc.TEST_SSIM])\n",
        "dc.metrics_tracker.plotMetrics(\"VGG\", [dc.TRAIN_VGG, dc.TEST_VGG])\n",
        "dc.metrics_tracker.plotMetrics(\"Unique Weights\", dc.metrics_tracker.getMetrics(dc.UNIQUE_WEIGHTS))\n",
        "dc.metrics_tracker.plotMetrics(\"Num Weights\", dc.metrics_tracker.getMetrics(dc.NUM_WEIGHTS))\n",
        "dc.metrics_tracker.plotMetrics(\"Sparsity\", dc.metrics_tracker.getMetrics(dc.SPARSITY), ylim=[0, 1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-429d37b7bc20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model, dataset = loadDecoderModelAndData(DATA_DIR + \"/HoneyBeePSNR/\", \n\u001b[1;32m      3\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                          nbit=8)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsnrLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfloating_point_bits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-355b9dc0ee95>\u001b[0m in \u001b[0;36mloadDecoderModelAndData\u001b[0;34m(folder_path, batch_size, nbit)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mcontaing\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mDataset\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"input.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m   \u001b[0mhuffman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSRHuffmanCoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/inputs_%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m   \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/Data/HoneyBeePSNR/input.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlJGfUdGxNUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################\n",
        "# Clear Directory\n",
        "##################\n",
        "clearTempFiles()\n",
        "model = dc.model\n",
        "# model, dataset = loadDecoderModelAndData(DATA_DIR + \"/\" + MODEL_PATH+ \"/\", \n",
        "#                                          batch_size=1, \n",
        "#                                          nbit=8)\n",
        "\n",
        "##################\n",
        "# Convert to CSR\n",
        "##################\n",
        "\n",
        "# Update if a different data type is desired for compression than was used for\n",
        "# training. Any reduction in floating_point_bits to the trained value will\n",
        "# likely result in a dergradation of model performance.\n",
        "floating_point_bits = floating_point_bits\n",
        "csr_weights, stats = modelToCsr(model, floating_point_bits)\n",
        "print(\"Dense: %d | CSR: %d | Reduction: %3f\" % stats)\n",
        "\n",
        "#####################################\n",
        "# Compress and Evaluate Model Sizes\n",
        "#####################################\n",
        "\n",
        "#########\n",
        "# Huffman\n",
        "##########\n",
        "huffman_coder = CSRHuffmanCoder()\n",
        "total_bytes = huffman_coder.encode(csr_weights)\n",
        "print(\"Huffman Code Size Kb: %0.f\" % total_bytes)\n",
        "loaded_weights =  huffman_coder.decode()\n",
        "dense_model = csrToModel(loaded_weights, tf.keras.models.clone_model(model),\n",
        "                         floating_point_bits)\n",
        "\n",
        "#########\n",
        "# Tf Model\n",
        "##########\n",
        "tf_path = '/tmp/tf_model_norm'\n",
        "tf.keras.models.save_model(dense_model, tf_path, include_optimizer=False)\n",
        "print (\"TF Save Size Kb: %0.f\" % getSizeRecursive(tf_path) )\n",
        "dense_model = tf.keras.models.load_model(tf_path)\n",
        "\n",
        "#########\n",
        "# Tf Lite\n",
        "#########\n",
        "# tf_path_8_bit = '/tmp/tf_model_8_bit.tflite'\n",
        "# converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# tflite_quant_model = converter.convert()\n",
        "# open(tf_path_8_bit, \"wb\").write(tflite_quant_model)\n",
        "# print (\"TF Lite Save Size Kb: %0.f\" % \n",
        "#        (os.path.getsize(tf_path_8_bit) / 1000.0))\n",
        "\n",
        "#########\n",
        "# Pickle\n",
        "#########\n",
        "pickle_path = '/tmp/pickle_model.pickle'\n",
        "with open(pickle_path, 'wb') as handle:\n",
        "  pickle.dump(csr_weights, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print (\"Pickle Save Size Kb: %0.f\" % (os.path.getsize(pickle_path) / 1000.0))\n",
        "\n",
        "################################\n",
        "# Decompress Model and Evaluate\n",
        "################################\n",
        "test_loss = tf.keras.metrics.Mean()\n",
        "test_psnr = tf.keras.metrics.Mean()\n",
        "test_ssim = tf.keras.metrics.Mean()\n",
        "test_vgg = tf.keras.metrics.Mean()\n",
        "\n",
        "pred = dense_model(dataset.x_test, training=False)\n",
        "loss = loss_fn(y_true=dataset.y_test, y_pred=pred)\n",
        "psnr = psnrLoss(dataset.y_test, pred, False)\n",
        "ssim = ssimLoss(dataset.y_test, pred, False)\n",
        "vgg = vggLoss(dataset.y_test, pred, False)\n",
        "\n",
        "test_loss.update_state(loss) \n",
        "test_psnr.update_state(psnr) \n",
        "test_ssim.update_state(ssim) \n",
        "test_vgg.update_state(vgg) \n",
        "\n",
        "print (\"TEST LOSS\", test_loss.result())\n",
        "print (\"TEST PSNR\", test_psnr.result())\n",
        "print (\"TEST SSIM\", test_ssim.result())\n",
        "print (\"TEST VGG\", test_vgg.result())\n",
        "plotFrames(dataset.y_test, pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r0LuELPuA4F",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Knowledge Distillation (Incomplete)\n",
        "# class KnowledgeDistiller():\n",
        "#   def __init__(self,\n",
        "#                student, \n",
        "#                teacher):\n",
        "#     self.student = student\n",
        "#     self.teacher = teacher\n",
        "\n",
        "#   def distillKnowledge(self):\n",
        "#     pass\n",
        "\n",
        "# def getStudentNetwork():\n",
        "#   model = tf.keras.Sequential()\n",
        "#   model.add(layers.Dense(64, activation='relu', input_shape=(32, 32, 3)))\n",
        "#   model.add(layers.Flatten())\n",
        "#   model.add(layers.Dense(64, activation='relu'))\n",
        "#   model.add(layers.Dense(64, activation='relu'))\n",
        "#   model.add(layers.Dense(10))\n",
        "#   return model\n",
        "\n",
        "# dataset = getDataset(64)\n",
        "# student = getStudentNetwork()\n",
        "# student.optimizer = tf.keras.optimizers.Adam()\n",
        "# teacher = dc.model\n",
        "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "# epochs = 10\n",
        "\n",
        "# for epoch in range(epochs):    \n",
        "#   step = 0\n",
        "#   train_loss = tf.keras.metrics.Mean()\n",
        "#   train_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "#   for i, (x_batch, y_batch) in enumerate(dataset.train_dataset):\n",
        "#     pred = None\n",
        "#     loss = None\n",
        "#     with tf.GradientTape() as tape:\n",
        "#       # Compute all loss metrics on the current training batch.\n",
        "#       pred_teacher = tf.nn.softmax(teacher(x_batch, training=True))\n",
        "#       pred_student = student(x_batch, training=True)\n",
        "#       loss = tf.nn.l2_loss(pred_student - pred_teacher)\n",
        "\n",
        "#     gradients = tape.gradient(loss, student.trainable_variables)\n",
        "#     student.optimizer.apply_gradients(zip(gradients, \n",
        "#                                           student.trainable_variables))\n",
        "#     train_loss.update_state(loss)\n",
        "#     train_acc.update_state(y_batch, student(x_batch, training=True))\n",
        "\n",
        "#     step += 1\n",
        "\n",
        "#   print (\"Epoch: %d | Step: %d | Train Loss: %d | Train Acc: %f\" % \n",
        "#           (epoch, step, train_loss.result(), train_acc.result()))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}